{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformer.modules import TransformerModule\n",
    "from transformer.tokenizer import get_tokenizer\n",
    "from transformer.dataset import TranslationDataset\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"data/wmt14_translate_de-en_train.csv\", lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>de</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>An der B 211 befindet sich in Loyermoor der so...</td>\n",
       "      <td>Here the largest town of the district is locat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ich begrüße die Erklärung des Herrn Kommissar ...</td>\n",
       "      <td>I should like, in passing, to pay tribute to t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Das ist das Gegenteil von dem, was getan werde...</td>\n",
       "      <td>That is the opposite of what should be done an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Ethnographical museum in Varna is in a hou...</td>\n",
       "      <td>It was designed by the Viennese architect Rupp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  de  \\\n",
       "0  An der B 211 befindet sich in Loyermoor der so...   \n",
       "1  Ich begrüße die Erklärung des Herrn Kommissar ...   \n",
       "2  Das ist das Gegenteil von dem, was getan werde...   \n",
       "3                                                  .   \n",
       "4  The Ethnographical museum in Varna is in a hou...   \n",
       "\n",
       "                                                  en  \n",
       "0  Here the largest town of the district is locat...  \n",
       "1  I should like, in passing, to pay tribute to t...  \n",
       "2  That is the opposite of what should be done an...  \n",
       "3                                                  .  \n",
       "4  It was designed by the Viennese architect Rupp...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4508785"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_lang = \"en\"\n",
    "source_lang = \"de\"\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"word_count\"] = dataset[source_lang].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11.02, 21.041]                 1739900\n",
       "(21.041, 31.061]                1033768\n",
       "(-1.9369999999999998, 11.02]     969093\n",
       "(31.061, 41.082]                 455500\n",
       "(41.082, 51.102]                 186776\n",
       "                                 ...   \n",
       "(1534.133, 1544.154]                  0\n",
       "(1544.154, 1554.174]                  0\n",
       "(1554.174, 1564.195]                  0\n",
       "(1564.195, 1574.215]                  0\n",
       "(1463.99, 1474.01]                    0\n",
       "Name: count, Length: 293, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# min = 1; max = 2937\n",
    "dataset[\"word_count\"].value_counts(\n",
    "    ascending=False,\n",
    "    bins=2937//10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word_count\n",
       "False    3723814\n",
       "True      784971\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"word_count\"].apply(\n",
    "    lambda x: x <= 10\n",
    ").value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sampling the dataset by Sentence Length and Sorting in Ascending Order\n",
    "dataset = dataset.loc[\n",
    "    dataset[\"word_count\"].apply(\n",
    "        lambda x: x <= 10\n",
    "    )\n",
    "].sort_values(\n",
    "    by=\"word_count\",\n",
    "    ascending=True\n",
    ").drop(columns=\"word_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dropping duplicates\n",
    "dataset = dataset.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentences -> Tokens (Split of Sentence into constituent components) -> Embedding (Every token becomes an embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import normalizers, pre_tokenizers, Tokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace, Digits, Punctuation\n",
    "from tokenizers.normalizers import NFD, Lowercase\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.models import WordLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(ds: pd.DataFrame, lang: str, tokenizer_path: str, vocab_size: int = 4096):\n",
    "    \n",
    "    assert lang in ds.columns, f\"{lang} should be a column in the dataset\"\n",
    "    \n",
    "    tokenizer= Tokenizer(WordLevel(unk_token=\"<unk>\"))\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Sequence([\n",
    "        Whitespace(),\n",
    "        Digits(individual_digits=True),\n",
    "        Punctuation()\n",
    "    ])\n",
    "    tokenizer.normalizer= normalizers.Sequence([NFD(), Lowercase()])\n",
    "    trainer = WordLevelTrainer(\n",
    "        vocab_size= vocab_size,\n",
    "        min_frequency= 5,\n",
    "        show_progress= True,\n",
    "        special_tokens= [\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"]\n",
    "    )\n",
    "    tokenizer.train_from_iterator(ds[lang].unique(), trainer= trainer)\n",
    "    tokenizer.save(tokenizer_path)\n",
    "    print(f\"Tokenizer saved at: {tokenizer_path}\")\n",
    "\n",
    "    return tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer_from_path(tokenizer_path: str):\n",
    "    return Tokenizer.from_file(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer saved at: ./tokenizer/de-en/en_vocab_16192.json\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(dataset, \"en\", \"./tokenizer/de-en/en_vocab_16192.json\", vocab_size=16192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 4826, 3, 4, 26, 51, 26, 51, 26, 51, 52, 50, 52, 50, 2, 0]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"<sos> neko is cat. 989898 6767 <eos> <pad>\").ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wir francis . vielen sein vielen sein vielen sein nr er nr er'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([1, 3, 9, 3888, 4, 49, 66, 49, 66, 49, 66, 68, 69, 68, 69, 2, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Dataset using `Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                ds: pd.DataFrame,\n",
    "                source_lang: str,\n",
    "                target_lang: str,\n",
    "                source_tokenizer_path: str,\n",
    "                target_tokenizer_path: str,\n",
    "                max_length: int,\n",
    "            ):\n",
    "        self.data = ds\n",
    "        self.max_length = max_length\n",
    "        self.source_lang = source_lang\n",
    "        self.target_lang = target_lang\n",
    "        self.source_tokenizer= get_tokenizer(source_tokenizer_path)\n",
    "        self.target_tokenizer= get_tokenizer(target_tokenizer_path)\n",
    "\n",
    "        self.sos_token = self.source_tokenizer.encode(\"<sos>\").ids\n",
    "        self.eos_token = self.source_tokenizer.encode(\"<eos>\").ids\n",
    "        self.pad_token = self.source_tokenizer.encode(\"<pad>\").ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        source_tokens = self.source_tokenizer.encode(row[self.source_lang]).ids[:self.max_length - 2]\n",
    "        target_tokens = self.target_tokenizer.encode(row[self.target_lang]).ids[:self.max_length - 2]\n",
    "\n",
    "        source_tokens = self.sos_token + source_tokens + self.eos_token\n",
    "        target_tokens = self.sos_token + target_tokens\n",
    "        label_tokens = target_tokens + self.eos_token\n",
    "\n",
    "        source_tokens += self.pad_token * (self.max_length - len(source_tokens))\n",
    "        target_tokens += self.pad_token * (self.max_length - len(target_tokens))\n",
    "        label_tokens += self.pad_token * (self.max_length - len(label_tokens))\n",
    "\n",
    "        # source tokens, target tokens, label tokens\n",
    "        return torch.Tensor(source_tokens).to(torch.int64), torch.Tensor(target_tokens).to(torch.int64), torch.Tensor(label_tokens).to(torch.int64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = TranslationDataset(\n",
    "    dataset,\n",
    "    \"eng\",\n",
    "    \"fr\",\n",
    "    \"./tokenizer/eng_vocab_4096.json\",\n",
    "    \"./tokenizer/fr_vocab_4096.json\",\n",
    "    64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = 0.7\n",
    "test_split = 0.2\n",
    "val_split = 0.1\n",
    "train, test, val = random_split(ds, [train_split, test_split, val_split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train, batch_size=32)\n",
    "test_dataloader = DataLoader(test, batch_size=32)\n",
    "val_dataloader = DataLoader(val, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Custom Function for yielding Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset:\n",
    "    def __init__(\n",
    "            self,\n",
    "            ds: pd.DataFrame,\n",
    "            source_lang: str,\n",
    "            target_lang: str,\n",
    "            source_tokenizer_path: str,\n",
    "            target_tokenizer_path: str,\n",
    "            batch_size: int,\n",
    "            max_length: int,\n",
    "        ):\n",
    "        \n",
    "        self.data = ds\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "        self.source_lang = source_lang\n",
    "        self.target_lang = target_lang\n",
    "        self.source_tokenizer= get_tokenizer_from_path(source_tokenizer_path)\n",
    "        self.target_tokenizer= get_tokenizer_from_path(target_tokenizer_path)\n",
    "\n",
    "        self.sos_token = self.source_tokenizer.encode(\"<sos>\").ids\n",
    "        self.eos_token = self.source_tokenizer.encode(\"<eos>\").ids\n",
    "        self.pad_token = self.source_tokenizer.encode(\"<pad>\").ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.data) // self.batch_size) + 1\n",
    "\n",
    "    def __iter__(self):\n",
    "        for idx in range(0, len(self.data), self.batch_size):\n",
    "            batch = self.data.iloc[idx: idx + self.batch_size]\n",
    "            source_tokens = batch[self.source_lang].apply(lambda x: self.source_tokenizer.encode(x).ids)\n",
    "            target_tokens = batch[self.target_lang].apply(lambda x: self.target_tokenizer.encode(x).ids)\n",
    "\n",
    "            source_max_length = source_tokens.apply(len).max()\n",
    "            target_max_length = target_tokens.apply(len).max()\n",
    "            max_length = target_max_length if target_max_length > source_max_length else source_max_length\n",
    "            max_length = self.max_length if max_length > self.max_length else max_length\n",
    "\n",
    "            source_tokens = source_tokens.apply(lambda x: self.sos_token + x[:max_length - 2] + self.eos_token)\n",
    "            target_tokens = target_tokens.apply(lambda x: self.sos_token + x[:max_length - 1])\n",
    "            label_tokens = target_tokens.apply(lambda x: x[:max_length - 1] + self.eos_token)\n",
    "\n",
    "            source_tokens = source_tokens.apply(lambda x: torch.Tensor(x + self.pad_token * (max_length - len(x))).to(torch.int64))\n",
    "            target_tokens = target_tokens.apply(lambda x: torch.Tensor(x + self.pad_token * (max_length - len(x))).to(torch.int64))\n",
    "            label_tokens = label_tokens.apply(lambda x: torch.Tensor(x + self.pad_token * (max_length - len(x))).to(torch.int64))\n",
    "\n",
    "            yield (\n",
    "                torch.stack(source_tokens.to_list()),\n",
    "                torch.stack(target_tokens.to_list()),\n",
    "                torch.stack(label_tokens.to_list())\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = TranslationDataset(\n",
    "    ds=dataset,\n",
    "    source_lang=\"de\",\n",
    "    target_lang=\"en\",\n",
    "    source_tokenizer_path=\"./tokenizer/de-en/de_vocab_16192.json\",\n",
    "    target_tokenizer_path=\"./tokenizer/de-en/en_vocab_16192.json\",\n",
    "    batch_size=64,\n",
    "    max_length=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerModule(\n",
    "    256,\n",
    "    16192,\n",
    "    16192,\n",
    "    128,\n",
    "    8,\n",
    "    6,\n",
    "    0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(torch.optim.lr_scheduler.LambdaLR):\n",
    "    def __init__(self, optimizer, d_model, warmup_steps=4000):\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "        super(CustomSchedule, self).__init__(optimizer, lr_lambda=self.lr_lambda)\n",
    "\n",
    "    def lr_lambda(self, step):\n",
    "        step = max(step, 1)\n",
    "        lr = (self.d_model ** -0.5) * min(step ** -0.5, step * (self.warmup_steps ** -1.5))\n",
    "        return lr\n",
    "\n",
    "\n",
    "target_tokenizer = get_tokenizer_from_path(\"./tokenizer/de-en/en_vocab_16192.json\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = CustomSchedule(optimizer, 256, 800)\n",
    "loss = nn.CrossEntropyLoss(ignore_index=target_tokenizer.token_to_id(\"<pad>\"), label_smoothing=0.1)\n",
    "\n",
    "state = torch.load(\"./checkpoints/16192-256/de_en_checkpoint-2\")\n",
    "model.load_state_dict(state['model_state_dict'])\n",
    "optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "scheduler.load_state_dict(state[\"scheduler_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch: 01: 100%|██████████| 11679/11679 [12:28:15<00:00,  3.84s/it, loss=5.796]    \n",
      "Processing Epoch: 02: 100%|██████████| 11679/11679 [15:50:18<00:00,  4.88s/it, loss=5.788]      \n",
      "Processing Epoch: 03:  42%|████▏     | 4923/11679 [5:43:40<7:51:38,  4.19s/it, loss=5.774] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m loss(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m16192\u001b[39m), label\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m      9\u001b[0m batch_iter\u001b[38;5;241m.\u001b[39mset_postfix({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m6.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m---> 11\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     13\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    527\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m _engine_run_backward(\n\u001b[0;32m    268\u001b[0m     tensors,\n\u001b[0;32m    269\u001b[0m     grad_tensors_,\n\u001b[0;32m    270\u001b[0m     retain_graph,\n\u001b[0;32m    271\u001b[0m     create_graph,\n\u001b[0;32m    272\u001b[0m     inputs,\n\u001b[0;32m    273\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    274\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 10):\n",
    "    model.train()\n",
    "    batch_iter = tqdm(train, desc= f\"Processing Epoch: {epoch:02d}\")\n",
    "    for batch in batch_iter:\n",
    "        output = model(batch[0], batch[1])\n",
    "        label = batch[2]\n",
    "\n",
    "        train_loss = loss(output.view(-1, 16192), label.view(-1))\n",
    "        batch_iter.set_postfix({\"loss\": f\"{train_loss.item():6.3f}\"})\n",
    "\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "        }, f\"./checkpoints/16192-256/de_en_checkpoint-{epoch}\" \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, source_tokenizer, target_tokenizer, input_sentence, max_length=32):\n",
    "    model.eval()\n",
    "    input_tokens = source_tokenizer.encode(input_sentence).ids\n",
    "    input_tensor = torch.tensor([input_tokens], dtype=torch.int64)\n",
    "\n",
    "    target_tokens = [target_tokenizer.token_to_id(\"<sos>\")]\n",
    "    target_tensor = torch.tensor([target_tokens], dtype=torch.int64)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            output = model(input_tensor, target_tensor)\n",
    "            next_token = output[0, -1, :].argmax(dim=-1).item()\n",
    "            target_tokens.append(next_token)\n",
    "            target_tensor = torch.tensor([target_tokens], dtype=torch.int64)\n",
    "\n",
    "            # if next_token == target_tokenizer.token_to_id(\"<eos>\"):\n",
    "            #     break\n",
    "\n",
    "    print(target_tokens)\n",
    "    output_sentence = target_tokenizer.decode(target_tokens)\n",
    "    return output_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"data/wmt14_translate_de-en_test.csv\", lineterminator='\\n')\n",
    "test[\"word_count\"] = test[source_lang].apply(lambda x: len(x.split()))\n",
    "test = test.loc[\n",
    "    test[\"word_count\"].apply(\n",
    "        lambda x: x <= 5\n",
    "    )\n",
    "].sort_values(\n",
    "    by=\"word_count\",\n",
    "    ascending=True\n",
    ").drop(columns=\"word_count\")\n",
    "test = test.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>de</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2625375</th>\n",
       "      <td>Warum?</td>\n",
       "      <td>Why is this?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51295</th>\n",
       "      <td>(Beifall)</td>\n",
       "      <td>(Applause)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3566197</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1677975</th>\n",
       "      <td>Beschäftigungsförderung</td>\n",
       "      <td>Community incentive measures in the field of e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1709050</th>\n",
       "      <td>Sie bedauert es, daß James Nichols im Gefängni...</td>\n",
       "      <td>It deplores the death of Mr Nichols in prison.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1709043</th>\n",
       "      <td>Sommerschulferien sind in der Tschechischen Re...</td>\n",
       "      <td>Summer school holidays in the Czech Republic a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1709029</th>\n",
       "      <td>Ich würde auch für die Einschaltung der nation...</td>\n",
       "      <td>I would also counsel in favour of including th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1709006</th>\n",
       "      <td>Auch Stirn, Hals, Decolleté, Schultern und Arm...</td>\n",
       "      <td>The forehead, neck, shoulders, sternum and arm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249052</th>\n",
       "      <td>Doch werden auch die unterschiedlichen Zwecke ...</td>\n",
       "      <td>But does everyone see and recognise the differ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>747415 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        de  \\\n",
       "3                                                        .   \n",
       "2625375                                             Warum?   \n",
       "51295                                            (Beifall)   \n",
       "3566197                                                  .   \n",
       "1677975                            Beschäftigungsförderung   \n",
       "...                                                    ...   \n",
       "1709050  Sie bedauert es, daß James Nichols im Gefängni...   \n",
       "1709043  Sommerschulferien sind in der Tschechischen Re...   \n",
       "1709029  Ich würde auch für die Einschaltung der nation...   \n",
       "1709006  Auch Stirn, Hals, Decolleté, Schultern und Arm...   \n",
       "2249052  Doch werden auch die unterschiedlichen Zwecke ...   \n",
       "\n",
       "                                                        en  \n",
       "3                                                        .  \n",
       "2625375                                       Why is this?  \n",
       "51295                                           (Applause)  \n",
       "3566197                                                  .  \n",
       "1677975  Community incentive measures in the field of e...  \n",
       "...                                                    ...  \n",
       "1709050     It deplores the death of Mr Nichols in prison.  \n",
       "1709043  Summer school holidays in the Czech Republic a...  \n",
       "1709029  I would also counsel in favour of including th...  \n",
       "1709006  The forehead, neck, shoulders, sternum and arm...  \n",
       "2249052  But does everyone see and recognise the differ...  \n",
       "\n",
       "[747415 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 14, 2, 3, 2, 2, 3, 3, 2, 3, 2, 14, 2, 14, 2, 2, 2, 2, 3, 2, 14, 4, 2, 2, 2, 14, 2, 2, 4, 4, 2, 14, 2, 2, 14, 2, 2, 14, 4, 2, 2, 4, 2, 14, 2, 2, 2, 4, 14, 4, 4, 4, 4, 4, 2, 2, 2, 2, 14, 2, 4, 2, 4, 3, 4, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 14, 2, 14, 2, 2, 4, 4, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 2, 4, 2, 14, 4, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 4, 2]\n",
      "0 0 0 0 . 0 . . 0 0 0 . . 0 . 0 . . . . . 0 . . . . 0 0 . . . . . . 0 . . .\n",
      "[1, 4, 2, 2, 2, 2, 2, 2, 4, 2, 4, 4, 4, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 14, 2, 2, 2, 14, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 14, 2, 2, 2, 14, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 14, 2, 2, 2, 4, 2, 14, 2, 2, 2, 2, 4, 14, 2, 2, 2, 2, 2, 14, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 14, 2, 3, 4, 14, 2, 2, 2, 2, 2, 2, 2, 2, 2, 14]\n",
      ". . . . . . . . 0 0 0 0 . 0 . 0 . 0 0 . 0 . 0 0\n",
      "[1, 4, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 2, 2, 2, 4, 2, 4, 2, 2, 2, 2, 2, 4, 2, 2, 2, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 14, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 14, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      ". . . . . . . . . . . . . . . 0 0\n",
      "[1, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 14, 2, 3, 2, 2, 3, 3, 2, 3, 2, 14, 2, 14, 2, 2, 2, 2, 3, 2, 14, 4, 2, 2, 2, 14, 2, 2, 4, 4, 2, 14, 2, 2, 14, 2, 2, 14, 4, 2, 2, 4, 2, 14, 2, 2, 2, 4, 14, 4, 4, 4, 4, 4, 2, 2, 2, 2, 14, 2, 4, 2, 4, 3, 4, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 14, 2, 14, 2, 2, 4, 4, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 2, 4, 2, 14, 4, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 4, 2]\n",
      "0 0 0 0 . 0 . . 0 0 0 . . 0 . 0 . . . . . 0 . . . . 0 0 . . . . . . 0 . . .\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, r \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(inference(\n\u001b[0;32m      3\u001b[0m         model,\n\u001b[0;32m      4\u001b[0m         get_tokenizer_from_path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./tokenizer/de-en/de_vocab_16192.json\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      5\u001b[0m         get_tokenizer_from_path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./tokenizer/de-en/en_vocab_16192.json\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      6\u001b[0m         r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mde\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;241m128\u001b[39m,\n\u001b[0;32m      8\u001b[0m     ))\n",
      "Cell \u001b[1;32mIn[28], line 11\u001b[0m, in \u001b[0;36minference\u001b[1;34m(model, source_tokenizer, target_tokenizer, input_sentence, max_length)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_length):\n\u001b[1;32m---> 11\u001b[0m         output \u001b[38;5;241m=\u001b[39m model(input_tensor, target_tensor)\n\u001b[0;32m     12\u001b[0m         next_token \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     13\u001b[0m         target_tokens\u001b[38;5;241m.\u001b[39mappend(next_token)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Programming\\Deep Learning\\Machine Learning Ground Up\\transformers\\transformer\\modules.py:155\u001b[0m, in \u001b[0;36mTransformerModule.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    153\u001b[0m decoder_output \u001b[38;5;241m=\u001b[39m output_embedding\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_layers:\n\u001b[1;32m--> 155\u001b[0m     decoder_output \u001b[38;5;241m=\u001b[39m decoder_layer(decoder_output, encoder_output, input_mask, target_mask)\n\u001b[0;32m    157\u001b[0m linear_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(decoder_output)\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m linear_output\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Programming\\Deep Learning\\Machine Learning Ground Up\\transformers\\transformer\\modules.py:111\u001b[0m, in \u001b[0;36mDecoderBlock.forward\u001b[1;34m(self, output_embeddings, encoder_output, source_mask, target_mask)\u001b[0m\n\u001b[0;32m    109\u001b[0m mmha_res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(mmha_res)\n\u001b[0;32m    110\u001b[0m add_norm1\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(torch\u001b[38;5;241m.\u001b[39madd(output_embeddings, mmha_res))\n\u001b[1;32m--> 111\u001b[0m mha_res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attention(add_norm1, encoder_output, encoder_output, source_mask)\n\u001b[0;32m    112\u001b[0m mha_res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(mha_res)\n\u001b[0;32m    113\u001b[0m add_norm2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(torch\u001b[38;5;241m.\u001b[39madd(add_norm1, mha_res))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Programming\\Deep Learning\\Machine Learning Ground Up\\transformers\\transformer\\modules.py:39\u001b[0m, in \u001b[0;36mMultiHeadAttentionWithMaskBlock.forward\u001b[1;34m(self, query, keys, values, mask)\u001b[0m\n\u001b[0;32m     36\u001b[0m     weights \u001b[38;5;241m=\u001b[39m weights\u001b[38;5;241m.\u001b[39mmasked_fill(mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1e9\u001b[39m)\n\u001b[0;32m     37\u001b[0m weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(weights, dim\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 39\u001b[0m weighted_values \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(weights, values)\n\u001b[0;32m     40\u001b[0m weighted_values \u001b[38;5;241m=\u001b[39m weighted_values\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mreshape(query\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim)\n\u001b[0;32m     41\u001b[0m attention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_layer(weighted_values)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, r in dataset.iterrows():\n",
    "    print(inference(\n",
    "        model,\n",
    "        get_tokenizer_from_path(\"./tokenizer/de-en/de_vocab_16192.json\"),\n",
    "        get_tokenizer_from_path(\"./tokenizer/de-en/en_vocab_16192.json\"),\n",
    "        r['de'],\n",
    "        128,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformer.modules import TransformerModule\n",
    "from transformer.tokenizer import get_tokenizer, load_tokenizer\n",
    "from transformer.dataset import DynamicBatchTranslationDataset\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"data/wmt14_translate_de-en_train.csv\", lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>de</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>An der B 211 befindet sich in Loyermoor der so...</td>\n",
       "      <td>Here the largest town of the district is locat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ich begrüße die Erklärung des Herrn Kommissar ...</td>\n",
       "      <td>I should like, in passing, to pay tribute to t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Das ist das Gegenteil von dem, was getan werde...</td>\n",
       "      <td>That is the opposite of what should be done an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Ethnographical museum in Varna is in a hou...</td>\n",
       "      <td>It was designed by the Viennese architect Rupp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  de  \\\n",
       "0  An der B 211 befindet sich in Loyermoor der so...   \n",
       "1  Ich begrüße die Erklärung des Herrn Kommissar ...   \n",
       "2  Das ist das Gegenteil von dem, was getan werde...   \n",
       "3                                                  .   \n",
       "4  The Ethnographical museum in Varna is in a hou...   \n",
       "\n",
       "                                                  en  \n",
       "0  Here the largest town of the district is locat...  \n",
       "1  I should like, in passing, to pay tribute to t...  \n",
       "2  That is the opposite of what should be done an...  \n",
       "3                                                  .  \n",
       "4  It was designed by the Viennese architect Rupp...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4508785"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_lang = \"en\"\n",
    "source_lang = \"de\"\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"word_count\"] = dataset[source_lang].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11.02, 21.041]                 1739900\n",
       "(21.041, 31.061]                1033768\n",
       "(-1.9369999999999998, 11.02]     969093\n",
       "(31.061, 41.082]                 455500\n",
       "(41.082, 51.102]                 186776\n",
       "                                 ...   \n",
       "(1534.133, 1544.154]                  0\n",
       "(1544.154, 1554.174]                  0\n",
       "(1554.174, 1564.195]                  0\n",
       "(1564.195, 1574.215]                  0\n",
       "(1463.99, 1474.01]                    0\n",
       "Name: count, Length: 293, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# min = 1; max = 2937\n",
    "dataset[\"word_count\"].value_counts(\n",
    "    ascending=False,\n",
    "    bins=2937//10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word_count\n",
       "False    3723814\n",
       "True      784971\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"word_count\"].apply(\n",
    "    lambda x: x <= 10\n",
    ").value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sampling the dataset by Sentence Length and Sorting in Ascending Order\n",
    "dataset = dataset.loc[\n",
    "    dataset[\"word_count\"].apply(\n",
    "        lambda x: x <= 10\n",
    "    )\n",
    "].sort_values(\n",
    "    by=\"word_count\",\n",
    "    ascending=True\n",
    ").drop(columns=\"word_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dropping duplicates\n",
    "dataset = dataset.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentences -> Tokens (Split of Sentence into constituent components) -> Embedding (Every token becomes an embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from tokenizers import normalizers, Tokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.normalizers import NFD, Lowercase\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.models import BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(\n",
    "        ds: pd.DataFrame,\n",
    "        lang: list,\n",
    "        tokenizer_path: str,\n",
    "        vocab_size: int = 32000\n",
    "    ):\n",
    "\n",
    "    for l in lang:\n",
    "        assert l in ds.columns, f\"{l} should be a column in the dataset\"\n",
    "    texts = pd.concat([ds[l] for l in lang]).astype(str).unique().tolist()\n",
    "    \n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
    "    tokenizer.normalizer = normalizers.Sequence([\n",
    "        NFD(),\n",
    "        Lowercase()\n",
    "    ])\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    trainer = BpeTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        special_tokens=[\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"]\n",
    "    )\n",
    "    tokenizer.train_from_iterator(texts, trainer=trainer)\n",
    "    tokenizer.save(tokenizer_path)\n",
    "    print(f\"Shared tokenizer saved at: {tokenizer_path}\")\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(tokenizer_path: str):\n",
    "    return Tokenizer.from_file(tokenizer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Shared tokenizer saved at: tokenizer/en_de_32000.json\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(\n",
    "    ds=dataset,\n",
    "    lang=[source_lang, target_lang],\n",
    "    tokenizer_path=\"tokenizer/en_de_32000.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1135, 1507, 1079, 3112, 17, 4212, 4212, 4212, 9376, 9376, 2, 0]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"<sos> neko is cat. 989898 6767 <eos> <pad>\").ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ne ko is cat . 98 98 98 67 67'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([1, 1135, 1507, 1079, 3112, 17, 4212, 4212, 4212, 9376, 9376, 2, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ne'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([1135])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Dataset using `Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                ds: pd.DataFrame,\n",
    "                source_lang: str,\n",
    "                target_lang: str,\n",
    "                source_tokenizer_path: str,\n",
    "                target_tokenizer_path: str,\n",
    "                max_length: int,\n",
    "            ):\n",
    "        self.data = ds\n",
    "        self.max_length = max_length\n",
    "        self.source_lang = source_lang\n",
    "        self.target_lang = target_lang\n",
    "        self.source_tokenizer= get_tokenizer(source_tokenizer_path)\n",
    "        self.target_tokenizer= get_tokenizer(target_tokenizer_path)\n",
    "\n",
    "        self.sos_token = self.source_tokenizer.encode(\"<sos>\").ids\n",
    "        self.eos_token = self.source_tokenizer.encode(\"<eos>\").ids\n",
    "        self.pad_token = self.source_tokenizer.encode(\"<pad>\").ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        source_tokens = self.source_tokenizer.encode(row[self.source_lang]).ids[:self.max_length - 2]\n",
    "        target_tokens = self.target_tokenizer.encode(row[self.target_lang]).ids[:self.max_length - 2]\n",
    "\n",
    "        source_tokens = self.sos_token + source_tokens + self.eos_token\n",
    "        target_tokens = self.sos_token + target_tokens\n",
    "        label_tokens = target_tokens + self.eos_token\n",
    "\n",
    "        source_tokens += self.pad_token * (self.max_length - len(source_tokens))\n",
    "        target_tokens += self.pad_token * (self.max_length - len(target_tokens))\n",
    "        label_tokens += self.pad_token * (self.max_length - len(label_tokens))\n",
    "\n",
    "        # source tokens, target tokens, label tokens\n",
    "        return torch.Tensor(source_tokens).to(torch.int64), torch.Tensor(target_tokens).to(torch.int64), torch.Tensor(label_tokens).to(torch.int64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = TranslationDataset(\n",
    "    dataset,\n",
    "    \"eng\",\n",
    "    \"fr\",\n",
    "    \"./tokenizer/eng_vocab_4096.json\",\n",
    "    \"./tokenizer/fr_vocab_4096.json\",\n",
    "    64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = 0.7\n",
    "test_split = 0.2\n",
    "val_split = 0.1\n",
    "train, test, val = random_split(ds, [train_split, test_split, val_split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train, batch_size=32)\n",
    "test_dataloader = DataLoader(test, batch_size=32)\n",
    "val_dataloader = DataLoader(val, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Custom Function for yielding Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicBatchTranslationDataset:\n",
    "    def __init__(\n",
    "            self,\n",
    "            ds: pd.DataFrame,\n",
    "            source_lang: str,\n",
    "            target_lang: str,\n",
    "            tokenizer_path: str,\n",
    "            batch_size: int,\n",
    "            max_length: int,\n",
    "        ):\n",
    "        \n",
    "        self.data = ds\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "        self.source_lang = source_lang\n",
    "        self.target_lang = target_lang\n",
    "        self.tokenizer= load_tokenizer(tokenizer_path)\n",
    "\n",
    "        self.sos_token = self.tokenizer.encode(\"<sos>\").ids\n",
    "        self.eos_token = self.tokenizer.encode(\"<eos>\").ids\n",
    "        self.pad_token = self.tokenizer.encode(\"<pad>\").ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.data) // self.batch_size) + 1\n",
    "\n",
    "    def __iter__(self):\n",
    "        for idx in range(0, len(self.data), self.batch_size):\n",
    "            batch = self.data.iloc[idx: idx + self.batch_size]\n",
    "            source_tokens = batch[self.source_lang].apply(lambda x: self.tokenizer.encode(x).ids)\n",
    "            target_tokens = batch[self.target_lang].apply(lambda x: self.tokenizer.encode(x).ids)\n",
    "\n",
    "            source_max_length = source_tokens.apply(len).max()\n",
    "            target_max_length = target_tokens.apply(len).max()\n",
    "            max_length = target_max_length if target_max_length > source_max_length else source_max_length\n",
    "            max_length = self.max_length if max_length > self.max_length else max_length\n",
    "\n",
    "            source_tokens = source_tokens.apply(lambda x: self.sos_token + x[:max_length - 2] + self.eos_token)\n",
    "            target_tokens = target_tokens.apply(lambda x: self.sos_token + x[:max_length - 1])\n",
    "            label_tokens = target_tokens.apply(lambda x: x[1: max_length] + self.eos_token)\n",
    "\n",
    "            source_tokens = source_tokens.apply(lambda x: torch.Tensor(x + self.pad_token * (max_length - len(x))).to(torch.int64))\n",
    "            target_tokens = target_tokens.apply(lambda x: torch.Tensor(x + self.pad_token * (max_length - len(x))).to(torch.int64))\n",
    "            label_tokens = label_tokens.apply(lambda x: torch.Tensor(x + self.pad_token * (max_length - len(x))).to(torch.int64))\n",
    "\n",
    "            yield (\n",
    "                torch.stack(source_tokens.to_list()),\n",
    "                torch.stack(target_tokens.to_list()),\n",
    "                torch.stack(label_tokens.to_list())\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=DynamicBatchTranslationDataset(dataset, \"de\", \"en\", \"tokenizer/en_de_32000.json\", 2, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = DynamicBatchTranslationDataset(\n",
    "    ds=dataset,\n",
    "    source_lang=\"de\",\n",
    "    target_lang=\"en\",\n",
    "    tokenizer_path=\"tokenizer/en_de_32000.json\",\n",
    "    batch_size=64,\n",
    "    max_length=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerModule(\n",
    "    dim=256,\n",
    "    vocab_size=32000,\n",
    "    max_token_length=128,\n",
    "    num_heads=8,\n",
    "    num_layers=6,\n",
    "    dropout=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(torch.optim.lr_scheduler.LambdaLR):\n",
    "    def __init__(self, optimizer, d_model, warmup_steps=4000):\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        super(CustomSchedule, self).__init__(optimizer, lr_lambda=self.lr_lambda)\n",
    "\n",
    "    def lr_lambda(self, step):\n",
    "        step = max(step, 1)\n",
    "        lr = (self.d_model ** -0.5) * min(step ** -0.5, step * (self.warmup_steps ** -1.5))\n",
    "        return lr\n",
    "\n",
    "\n",
    "tokenizer = load_tokenizer(\"tokenizer/en_de_32000.json\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = CustomSchedule(optimizer, 256, 800)\n",
    "loss = nn.CrossEntropyLoss(ignore_index=tokenizer.token_to_id(\"<pad>\"), label_smoothing=0.1)\n",
    "\n",
    "# state = torch.load(\"./checkpoints/de_en_run3/de_en_checkpoint-19\")\n",
    "# model.load_state_dict(state['model_state_dict'])\n",
    "# optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "# scheduler.load_state_dict(state[\"scheduler_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"./checkpoints/de_en_run3\"\n",
    "if not os.path.exists(save_dir):\n",
    "    print(f\"Directory {save_dir} does not exist. Please create it before starting training.\")\n",
    "else:\n",
    "    for epoch in range(20, 21):\n",
    "        model.train()\n",
    "        batch_iter = tqdm(train, desc=f\"Processing Epoch: {epoch:02d}\")\n",
    "        for batch in batch_iter:\n",
    "            output = model(batch[0], batch[1])\n",
    "            label = batch[2]\n",
    "\n",
    "            train_loss = loss(output.view(-1, 32000), label.view(-1))\n",
    "            batch_iter.set_postfix({\"loss\": f\"{train_loss.item():6.3f}\"})\n",
    "\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "            }, f\"{save_dir}/de_en_checkpoint-{epoch}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, tokenizer, input_sentence, max_length=32):\n",
    "    model.eval()\n",
    "    input_tokens = tokenizer.encode(input_sentence).ids\n",
    "    input_tensor = torch.tensor([input_tokens], dtype=torch.int64)\n",
    "\n",
    "    target_tokens = [tokenizer.token_to_id(\"<sos>\")]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            target_tensor = torch.tensor([target_tokens], dtype=torch.int64)\n",
    "            output = model(input_tensor, target_tensor)\n",
    "            next_token = output[0, -1, :].argmax(dim=-1).item()\n",
    "            target_tokens.append(next_token)\n",
    "\n",
    "            if next_token == tokenizer.token_to_id(\"<eos>\"):\n",
    "                break\n",
    "\n",
    "    output_sentence = tokenizer.decode(target_tokens)\n",
    "    return output_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"data/wmt14_translate_de-en_test.csv\", lineterminator='\\n')\n",
    "test[\"word_count\"] = test[source_lang].apply(lambda x: len(x.split()))\n",
    "test = test.loc[\n",
    "    test[\"word_count\"].apply(\n",
    "        lambda x: x <= 5\n",
    "    )\n",
    "].sort_values(\n",
    "    by=\"word_count\",\n",
    "    ascending=True\n",
    ").drop(columns=\"word_count\")\n",
    "test = test.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>de</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1770</th>\n",
       "      <td>Expressofan</td>\n",
       "      <td>Espresso fan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>Ferne Welten.</td>\n",
       "      <td>Distant worlds.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>Wiederkehr feiern.</td>\n",
       "      <td>July.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2723</th>\n",
       "      <td>Hundefreunde erfolgreich</td>\n",
       "      <td>Dog-lovers victorious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>Das Töten.</td>\n",
       "      <td>Killing.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228</th>\n",
       "      <td>Alles für ein unvergessliches Fest.</td>\n",
       "      <td>Everything you need for an unforgettable celeb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1216</th>\n",
       "      <td>Freudenstadt: Schnelle Aktionen überrumpeln Ga...</td>\n",
       "      <td>Freudenstadt: Quick moves take hosts by surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186</th>\n",
       "      <td>Wir fürchten, das begünstigt Interventionen.</td>\n",
       "      <td>We are afraid it will encourage intervention.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1941</th>\n",
       "      <td>Autofahrer bei Unfall schwer verletzt</td>\n",
       "      <td>Car driver seriously injured in accident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2969</th>\n",
       "      <td>CDC veröffentlichen Allergierichtlinien für Sc...</td>\n",
       "      <td>CDC issues children's allergy guidelines for s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     de  \\\n",
       "1770                                        Expressofan   \n",
       "979                                       Ferne Welten.   \n",
       "1967                                 Wiederkehr feiern.   \n",
       "2723                           Hundefreunde erfolgreich   \n",
       "345                                          Das Töten.   \n",
       "...                                                 ...   \n",
       "1228                Alles für ein unvergessliches Fest.   \n",
       "1216  Freudenstadt: Schnelle Aktionen überrumpeln Ga...   \n",
       "1186       Wir fürchten, das begünstigt Interventionen.   \n",
       "1941              Autofahrer bei Unfall schwer verletzt   \n",
       "2969  CDC veröffentlichen Allergierichtlinien für Sc...   \n",
       "\n",
       "                                                     en  \n",
       "1770                                       Espresso fan  \n",
       "979                                     Distant worlds.  \n",
       "1967                                              July.  \n",
       "2723                              Dog-lovers victorious  \n",
       "345                                            Killing.  \n",
       "...                                                 ...  \n",
       "1228  Everything you need for an unforgettable celeb...  \n",
       "1216   Freudenstadt: Quick moves take hosts by surprise  \n",
       "1186      We are afraid it will encourage intervention.  \n",
       "1941           Car driver seriously injured in accident  \n",
       "2969  CDC issues children's allergy guidelines for s...  \n",
       "\n",
       "[111 rows x 2 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, r in test.iterrows():\n",
    "    print(\"Actual Sentence: \", r[source_lang])\n",
    "    print(\"Target Sentence: \", r[target_lang])\n",
    "    print(\"Predicted Sentence: \", inference(\n",
    "        model,\n",
    "        load_tokenizer(\"tokenizer/en_de_32000.json\"),\n",
    "        r[source_lang],\n",
    "        128,\n",
    "    ))\n",
    "    print(\"---------\"*10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

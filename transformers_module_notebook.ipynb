{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## self- attention is basically doing cosine similarity across the embeddings of every word (word+ position vector)\n",
    "## first, all(vectors) -> all(vectors) cosine similarity\n",
    "## for instance, to find the first word's embedding: we find the cosine similarity of it with every other word \n",
    "## (that is the affinity of the word to every other word. One with high relation should be given more priority)\n",
    "## So, this cosine similarity we have here would be taken as a weight for every other word we have\n",
    "## But now the dimension of the embedding is (n, x) where n is the number of words, We must reduce it to (1, x) \n",
    "## We can add all the embeddings multiplied with their weight to get the embedding of the word\n",
    "\n",
    "## _but wait, there is nothing being trained though. How will it learn_\n",
    "\n",
    "## Query, Key, Values\n",
    "## Query - the vector in question for which we are finding the embedding is called the query\n",
    "## Key- All the vectors with which it finds a similarity\n",
    "## Values- All the vectors again, when they are about to be multiplied with weights\n",
    "\n",
    "## Question on my mind: I understand that we are finding the dot products of a vector with another vector. Doesn't the basic definition of \n",
    "## cosine similarity mean that we are seeing if two words are alike. For instance, \"dog\" and \"bark\". \n",
    "## So, for a change if the sentence we are looking at is: \"The dog was barked at by the human\"\n",
    "## \"human\" and \"bark\" will have less similarity, as compared to the \"dog\" and \"bark\" when this situation actually calls for it\n",
    "## The idea of the attention block is to learn as many Natural Language properties as possible\n",
    "## Does a different function work then? As long as it's a static way of computing it can't compute relations on the fly. can it?\n",
    "## There must be a learning for sure, then\n",
    "## why did we decide for it to happen on query, key and values? and why linear layers?\n",
    "## what do linear layers learn to do? they learn a combination of weights so that the resultant vector is a representation of the dynamic ways\n",
    "## the input can be interpreted\n",
    "\n",
    "## reverse engineer: if the dot product we are finding now, would indeed increase the similarity for \"human\" and \"bark\". \n",
    "## if cosine similarity remained the same, then? it's the vectors which have been modified. So what the FC layers essentially do is massage\n",
    "## the vectors so that it is tuned to produce an attention vector for a particular NL property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(arr: np.ndarray):\n",
    "    sum_of_exponents = np.sum(np.exp(arr))\n",
    "    return arr/ sum_of_exponents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_embedding = np.random.randn(10, 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention Block without Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_block_without_linear_layers(sentence: np.ndarray):\n",
    "    query= sentence.copy()\n",
    "    keys = sentence.copy()\n",
    "    values = sentence.copy()\n",
    "\n",
    "    weights = cosine_similarity(query, keys)\n",
    "    weights = softmax(weights)\n",
    "    weighted_values = np.matmul(weights, values)\n",
    "    return weighted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_block_without_linear_layers(sent_embedding).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_block(sentence: np.ndarray):\n",
    "    #query\n",
    "    query = Tensor(sentence.copy())\n",
    "    query_layer = nn.Linear(1024, 1024)\n",
    "    query = query_layer(query)\n",
    "\n",
    "    #keys\n",
    "    keys = Tensor(sentence.copy())\n",
    "    keys_layer = nn.Linear(1024, 1024)\n",
    "    keys = keys_layer(keys)\n",
    "\n",
    "    #values\n",
    "    values = Tensor(sentence.copy())\n",
    "    values_layer = nn.Linear(1024, 1024)\n",
    "    values = values_layer(values)\n",
    "\n",
    "    weights = nn.CosineSimilarity()(query, keys)\n",
    "    weights = nn.Softmax(weights).dim\n",
    "    weighted_values = torch.matmul(weights, values)\n",
    "    return weighted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_block(sent_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, dim= 1024):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "\n",
    "        self.query_layer = nn.Linear(dim, dim)\n",
    "        self.keys_layer = nn.Linear(dim, dim)\n",
    "        self.values_layer = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "\n",
    "        if not isinstance(sentence, Tensor):\n",
    "            sentence= Tensor(sentence)\n",
    "\n",
    "        query= self.query_layer(sentence)\n",
    "        keys= self.keys_layer(sentence)\n",
    "        values= self.values_layer(sentence)\n",
    "\n",
    "        weights = F.cosine_similarity(query.unsqueeze(1), keys.unsqueeze(0), dim=-1)\n",
    "        weights = F.softmax(weights, dim= -1)\n",
    "\n",
    "        weighted_values = torch.matmul(weights, values)\n",
    "\n",
    "        return weighted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0299, -0.0435, -0.1451,  ..., -0.4967,  0.2968,  0.0756],\n",
       "        [ 0.0259, -0.0464, -0.1332,  ..., -0.4950,  0.2985,  0.0762],\n",
       "        [ 0.0272, -0.0442, -0.1294,  ..., -0.4916,  0.2944,  0.0765],\n",
       "        ...,\n",
       "        [ 0.0281, -0.0411, -0.1251,  ..., -0.4970,  0.2939,  0.0725],\n",
       "        [ 0.0288, -0.0370, -0.1393,  ..., -0.4963,  0.2989,  0.0747],\n",
       "        [ 0.0361, -0.0275, -0.1311,  ..., -0.4937,  0.3030,  0.0620]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att= AttentionBlock()\n",
    "att.forward(sent_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi Head Attention Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self, dim= 1024, num_heads= 4):\n",
    "        super(MultiHeadAttentionBlock, self).__init__()\n",
    "\n",
    "        assert dim%num_heads==0, \"dim should be divisible by num_heads\"\n",
    "\n",
    "        self.dim= dim\n",
    "        self.heads = num_heads\n",
    "        self.per_head = dim // num_heads\n",
    "\n",
    "        self.query_layer = nn.Linear(dim, dim)\n",
    "        self.keys_layer = nn.Linear(dim, dim)\n",
    "        self.values_layer = nn.Linear(dim, dim)\n",
    "        self.linear_layer = nn.Linear(dim, dim)\n",
    "\n",
    "    def split_head(self, tensor: Tensor):\n",
    "        batch_size, num_tokens, dim = tensor.size()\n",
    "        return tensor.view(batch_size, num_tokens, self.heads, self.per_head).transpose(1, 2)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "\n",
    "        if not isinstance(sentence, Tensor):\n",
    "            sentence= Tensor(sentence)\n",
    "\n",
    "        query= self.split_head(self.query_layer(sentence))\n",
    "        keys= self.split_head(self.keys_layer(sentence))\n",
    "        values= self.split_head(self.values_layer(sentence))\n",
    "\n",
    "        weights = F.cosine_similarity(query.unsqueeze(3), keys.unsqueeze(2), dim=-1) / math.sqrt(self.per_head)\n",
    "        weights = F.softmax(weights, dim= -1)\n",
    "\n",
    "        weighted_values = torch.matmul(weights, values)\n",
    "        weighted_values = weighted_values.transpose(1,2).contiguous().reshape(sentence.shape[0], -1, self.dim)\n",
    "        attention = self.linear_layer(weighted_values)\n",
    "\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 1024])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_embedding = np.random.rand(1, 10, 1024)\n",
    "mha = MultiHeadAttentionBlock()\n",
    "mha.forward(sent_embedding).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masked Multi-Head Attention Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWithMaskBlock(nn.Module):\n",
    "    def __init__(self, dim= 1024, num_heads= 4):\n",
    "        super(MultiHeadAttentionWithMaskBlock, self).__init__()\n",
    "\n",
    "        assert dim%num_heads==0, \"dim should be divisible by num_heads\"\n",
    "\n",
    "        self.dim= dim\n",
    "        self.heads = num_heads\n",
    "        self.per_head = dim // num_heads\n",
    "\n",
    "        self.query_layer = nn.Linear(dim, dim)\n",
    "        self.keys_layer = nn.Linear(dim, dim)\n",
    "        self.values_layer = nn.Linear(dim, dim)\n",
    "        self.linear_layer = nn.Linear(dim, dim)\n",
    "\n",
    "    def split_head(self, tensor: Tensor):\n",
    "        batch_size, num_tokens, dim = tensor.size()\n",
    "        return tensor.view(batch_size, num_tokens, self.heads, self.per_head).transpose(1, 2)\n",
    "\n",
    "    def forward(self, query, keys, values, mask= None):\n",
    "\n",
    "        query= self.split_head(self.query_layer(query))\n",
    "        keys= self.split_head(self.keys_layer(keys))\n",
    "        values= self.split_head(self.values_layer(values))\n",
    "\n",
    "        weights = F.cosine_similarity(query.unsqueeze(3), keys.unsqueeze(2), dim=-1) / math.sqrt(self.per_head)\n",
    "        if mask is not None:\n",
    "            weights = weights.masked_fill(mask == 0, -1e9)\n",
    "        weights = F.softmax(weights, dim= -2)\n",
    "\n",
    "        weighted_values = torch.matmul(weights, values)\n",
    "        weighted_values = weighted_values.transpose(1,2).contiguous().reshape(query.shape[0], -1, self.dim)\n",
    "        attention = self.linear_layer(weighted_values)\n",
    "\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7044, 0.1143, 0.3322, 0.7135, 0.7482, 0.1063, 0.8904, 0.2262, 0.0000,\n",
       "         0.0000],\n",
       "        [0.3648, 0.7871, 0.4774, 0.5742, 0.3992, 0.6542, 0.3569, 0.0488, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0378, 0.5281, 0.1939, 0.1050, 0.5639, 0.4289, 0.5317, 0.2887, 0.0000,\n",
       "         0.0000]])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_embedding = Tensor(np.random.rand(3, 10, 1024))\n",
    "sent= Tensor(np.random.rand(3, 10))\n",
    "sent[:, 8:] = 0\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]],\n",
       "\n",
       "\n",
       "        [[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]],\n",
       "\n",
       "\n",
       "        [[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]]])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = (sent != 0).unsqueeze(1).unsqueeze(2)\n",
    "tri = torch.tril(torch.ones(3, 1, 10, 10))\n",
    "torch.logical_and(mask, tri).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 10, 10])\n",
      "torch.Size([3, 4, 10, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2608,  0.0836,  0.0352,  ..., -0.0335,  0.1987, -0.2264],\n",
       "         [-0.2605,  0.0835,  0.0348,  ..., -0.0333,  0.1985, -0.2263],\n",
       "         [-0.2603,  0.0836,  0.0346,  ..., -0.0330,  0.1986, -0.2266],\n",
       "         ...,\n",
       "         [-0.2610,  0.0834,  0.0351,  ..., -0.0332,  0.1986, -0.2264],\n",
       "         [-0.2608,  0.0835,  0.0350,  ..., -0.0332,  0.1985, -0.2265],\n",
       "         [-0.2607,  0.0835,  0.0350,  ..., -0.0333,  0.1985, -0.2263]],\n",
       "\n",
       "        [[-0.3146,  0.0826,  0.0528,  ...,  0.0040,  0.2743, -0.2550],\n",
       "         [-0.3145,  0.0827,  0.0525,  ...,  0.0041,  0.2743, -0.2550],\n",
       "         [-0.3147,  0.0827,  0.0527,  ...,  0.0040,  0.2745, -0.2551],\n",
       "         ...,\n",
       "         [-0.3147,  0.0827,  0.0524,  ...,  0.0043,  0.2742, -0.2551],\n",
       "         [-0.3150,  0.0828,  0.0526,  ...,  0.0042,  0.2746, -0.2554],\n",
       "         [-0.3149,  0.0827,  0.0526,  ...,  0.0041,  0.2744, -0.2552]],\n",
       "\n",
       "        [[-0.3039,  0.0498,  0.1231,  ..., -0.0834,  0.2766, -0.2127],\n",
       "         [-0.3043,  0.0496,  0.1235,  ..., -0.0841,  0.2769, -0.2122],\n",
       "         [-0.3042,  0.0499,  0.1232,  ..., -0.0837,  0.2769, -0.2128],\n",
       "         ...,\n",
       "         [-0.3040,  0.0499,  0.1230,  ..., -0.0835,  0.2770, -0.2127],\n",
       "         [-0.3040,  0.0498,  0.1231,  ..., -0.0838,  0.2766, -0.2124],\n",
       "         [-0.3046,  0.0497,  0.1236,  ..., -0.0843,  0.2772, -0.2122]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mham = MultiHeadAttentionWithMaskBlock()\n",
    "res = mham.forward(sent_embedding, sent_embedding, sent_embedding, mask)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetworkBlock(nn.Module):\n",
    "    def __init__(self, dim= 1024, inter_dim= 512):\n",
    "        super(FeedForwardNetworkBlock, self).__init__()\n",
    "\n",
    "        self.ff1 = nn.Linear(dim, inter_dim)\n",
    "        self.ff2 = nn.Linear(inter_dim, dim)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, attention):\n",
    "        return self.ff2(self.relu(self.ff1(attention)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0311, -0.0649, -0.0714,  ..., -0.0334,  0.0045,  0.0153],\n",
       "         [ 0.0311, -0.0649, -0.0714,  ..., -0.0334,  0.0045,  0.0154],\n",
       "         [ 0.0311, -0.0649, -0.0714,  ..., -0.0334,  0.0045,  0.0153],\n",
       "         ...,\n",
       "         [ 0.0311, -0.0649, -0.0714,  ..., -0.0334,  0.0045,  0.0154],\n",
       "         [ 0.0311, -0.0649, -0.0714,  ..., -0.0334,  0.0045,  0.0154],\n",
       "         [ 0.0311, -0.0649, -0.0714,  ..., -0.0334,  0.0045,  0.0153]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn = FeedForwardNetworkBlock()\n",
    "ffn.forward(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncodingBlock(nn.Module):\n",
    "    def __init__(self, max_token_length, dim):\n",
    "        super(PositionalEncodingBlock, self).__init__()\n",
    "\n",
    "        pe= torch.zeros(max_token_length, dim)\n",
    "        position = torch.arange(0, max_token_length, dtype=torch.float).unsqueeze(1)\n",
    "        # div_alt = torch.exp(torch.arange(0, dim, 2).float() * -(math.log(10000.0) / dim))\n",
    "        div = 1 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div)\n",
    "        pe[:, 1::2] = torch.cos(position * div)\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        return tokens + self.pe[:, :tokens.size(1), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8690,  1.1486,  0.8891,  ...,  1.7113,  0.5925,  1.1869],\n",
       "         [ 1.2769,  1.1276,  1.4926,  ...,  1.1220,  0.9188,  1.7056],\n",
       "         [ 1.1858,  0.5714,  1.5746,  ...,  1.3050,  0.7084,  1.8731],\n",
       "         ...,\n",
       "         [ 0.7074,  1.0905,  0.7942,  ...,  1.0692,  0.1868,  1.2570],\n",
       "         [ 1.8714,  0.6506,  1.3127,  ...,  1.9354,  0.8969,  1.8366],\n",
       "         [ 1.1768, -0.7912,  0.6091,  ...,  1.4076,  0.4606,  1.5468]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe= PositionalEncodingBlock(100, 1024)\n",
    "pe.forward(sent_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, num_heads: int = 4, dim: int = 1024, inter_dim: int = 512):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttentionWithMaskBlock(dim, num_heads)\n",
    "        self.ff = FeedForwardNetworkBlock(dim, inter_dim)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, token_embeddings, input_mask):\n",
    "        mha_res = self.mha(token_embeddings, token_embeddings, token_embeddings, input_mask)\n",
    "        add_norm1= self.norm1(torch.add(token_embeddings, mha_res))\n",
    "        ff_res = self.ff(add_norm1)\n",
    "        add_norm2 = self.norm2(torch.add(add_norm1, ff_res))\n",
    "        return add_norm2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, num_heads: int = 4, dim: int = 1024, inter_dim: int = 512):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        self.self_attention = MultiHeadAttentionWithMaskBlock(dim, num_heads)\n",
    "        self.cross_attention = MultiHeadAttentionWithMaskBlock(dim, num_heads)\n",
    "        self.ff = FeedForwardNetworkBlock(dim, inter_dim)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.norm3 = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, output_embeddings, encoder_output, source_mask, target_mask):\n",
    "        mmha_res = self.self_attention(output_embeddings, output_embeddings, output_embeddings, target_mask)\n",
    "        add_norm1= self.norm1(torch.add(output_embeddings, mmha_res))\n",
    "        mha_res = self.cross_attention(add_norm1, encoder_output, encoder_output, source_mask)\n",
    "        add_norm2 = self.norm2(torch.add(mmha_res, mha_res))\n",
    "        ff_res = self.ff(add_norm2)\n",
    "        add_norm3 = self.norm2(torch.add(add_norm2, ff_res))\n",
    "        return add_norm3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_embedding = Tensor(np.random.randn(3, 10, 1024))\n",
    "sent= torch.rand(3, 10)\n",
    "enc_embedding = Tensor(np.random.randn(3, 12, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mask = (sent != 0).unsqueeze(1).unsqueeze(2)\n",
    "batch_size, sequence_length= sent.size()\n",
    "tri = torch.tril(torch.ones(batch_size, 1, sequence_length, sequence_length))\n",
    "torch.logical_and(target_mask, tri).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mask= torch.ones(1, 1, sent_embedding.size(1), 1)\n",
    "target_mask[:, :, out_embedding.size(1):, :] = 0\n",
    "source_mask= torch.zeros(1, 1, sent_embedding.size(1), 1)\n",
    "source_mask[:, :, out_embedding.size(1):, :] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 10, 10])\n",
      "torch.Size([1, 4, 10, 10])\n",
      "torch.Size([1, 4, 10, 10])\n",
      "torch.Size([1, 4, 10, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1505, -1.4318, -0.8466,  ..., -0.7459,  1.2129,  1.6794],\n",
       "         [ 0.1497, -1.4256, -0.8490,  ..., -0.7419,  1.2107,  1.6744],\n",
       "         [ 0.1459, -1.4208, -0.8456,  ..., -0.7463,  1.2139,  1.6787],\n",
       "         ...,\n",
       "         [ 0.7678, -0.9228,  0.2006,  ..., -1.3777,  0.2824, -0.8879],\n",
       "         [ 0.7692, -0.9339,  0.2005,  ..., -1.3825,  0.2834, -0.8881],\n",
       "         [ 0.7591, -0.9308,  0.2029,  ..., -1.3822,  0.2785, -0.8891]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec= DecoderBlock()\n",
    "dec(sent_embedding, sent_embedding, source_mask, target_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModule(nn.Module):\n",
    "    def __init__(self, dim, encoder_vocab_size, decoder_vocab_size, max_token_length, num_heads, num_layers):\n",
    "        super(TransformerModule, self).__init__()\n",
    "\n",
    "        self.encoder_embedding = nn.Embedding(encoder_vocab_size, dim)\n",
    "        self.decoder_embedding = nn.Embedding(decoder_vocab_size, dim)\n",
    "        self.positional_encoder = PositionalEncodingBlock(max_token_length, dim)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderBlock(num_heads, dim, 2* dim) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderBlock(num_heads, dim, 2* dim) for _ in range(num_layers)])\n",
    "        \n",
    "        self.linear = nn.Linear(dim, decoder_vocab_size)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def generate_mask(self, input, target):\n",
    "        input_mask = (input != 0).unsqueeze(1).unsqueeze(2)\n",
    "        target_mask = (target != 0).unsqueeze(1).unsqueeze(2)\n",
    "        batch_size, sequence_length= target.size()\n",
    "        tri = torch.tril(torch.ones(batch_size, 1, sequence_length, sequence_length))\n",
    "        target_mask = torch.logical_and(target_mask, tri).long()\n",
    "\n",
    "        return input_mask, target_mask\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        input_mask, target_mask = self.generate_mask(input, target)\n",
    "\n",
    "        input_embedding = self.encoder_embedding(input)\n",
    "        output_embedding = self.decoder_embedding(target)\n",
    "\n",
    "        encoder_output = input_embedding\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            encoder_output = encoder_layer(encoder_output, input_mask)\n",
    "\n",
    "        decoder_output = output_embedding\n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            decoder_output = decoder_layer(decoder_output, encoder_output, input_mask, target_mask)\n",
    "\n",
    "        linear_output = self.linear(decoder_output)\n",
    "        return self.softmax(linear_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TransformerModule(\n",
    "    dim= 64,\n",
    "    encoder_vocab_size=500,\n",
    "    decoder_vocab_size=500,\n",
    "    max_token_length=100,\n",
    "    num_heads=4,\n",
    "    num_layers=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harisha\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1532: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2345, 0.3223, 0.3191,  ..., 0.2616, 0.2889, 0.2379],\n",
       "         [0.2345, 0.3218, 0.3178,  ..., 0.2617, 0.2894, 0.2377],\n",
       "         [0.2346, 0.3212, 0.3163,  ..., 0.2618, 0.2896, 0.2378],\n",
       "         ...,\n",
       "         [0.2552, 0.1833, 0.2444,  ..., 0.2038, 0.3027, 0.2636],\n",
       "         [0.2673, 0.1612, 0.2405,  ..., 0.1898, 0.3266, 0.2705],\n",
       "         [0.2797, 0.1431, 0.2414,  ..., 0.1703, 0.3629, 0.2758]],\n",
       "\n",
       "        [[0.2566, 0.2335, 0.2245,  ..., 0.2718, 0.2672, 0.2174],\n",
       "         [0.2578, 0.2329, 0.2252,  ..., 0.2724, 0.2671, 0.2183],\n",
       "         [0.2594, 0.2322, 0.2258,  ..., 0.2732, 0.2674, 0.2189],\n",
       "         ...,\n",
       "         [0.3828, 0.2000, 0.2150,  ..., 0.3144, 0.3016, 0.2409],\n",
       "         [0.3703, 0.2111, 0.2139,  ..., 0.3292, 0.2913, 0.2274],\n",
       "         [0.3740, 0.2112, 0.1952,  ..., 0.3746, 0.2602, 0.2187]],\n",
       "\n",
       "        [[0.2387, 0.2130, 0.2173,  ..., 0.2352, 0.2043, 0.2951],\n",
       "         [0.2385, 0.2136, 0.2176,  ..., 0.2357, 0.2040, 0.2950],\n",
       "         [0.2384, 0.2145, 0.2178,  ..., 0.2360, 0.2036, 0.2948],\n",
       "         ...,\n",
       "         [0.1898, 0.3600, 0.2724,  ..., 0.2791, 0.1802, 0.2300],\n",
       "         [0.1846, 0.3730, 0.2860,  ..., 0.2748, 0.1584, 0.2348],\n",
       "         [0.1786, 0.3811, 0.2938,  ..., 0.2519, 0.1243, 0.2486]],\n",
       "\n",
       "        [[0.2703, 0.2312, 0.2391,  ..., 0.2314, 0.2396, 0.2496],\n",
       "         [0.2691, 0.2316, 0.2394,  ..., 0.2302, 0.2395, 0.2490],\n",
       "         [0.2676, 0.2321, 0.2400,  ..., 0.2291, 0.2395, 0.2485],\n",
       "         ...,\n",
       "         [0.1721, 0.2568, 0.2681,  ..., 0.2028, 0.2155, 0.2655],\n",
       "         [0.1778, 0.2547, 0.2595,  ..., 0.2062, 0.2238, 0.2673],\n",
       "         [0.1677, 0.2646, 0.2696,  ..., 0.2032, 0.2526, 0.2569]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sentences = torch.randint(1, 500, (4, 100))\n",
    "target_sentences = torch.randint(1, 500, (4, 100))\n",
    "transformer(input_sentences, target_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

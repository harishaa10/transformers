{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(arr: np.ndarray):\n",
    "    sum_of_exponents = np.sum(np.exp(arr))\n",
    "    return arr/ sum_of_exponents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_embedding = np.random.randn(10, 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention Block without Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_block_without_linear_layers(sentence: np.ndarray):\n",
    "    query= sentence.copy()\n",
    "    keys = sentence.copy()\n",
    "    values = sentence.copy()\n",
    "\n",
    "    weights = cosine_similarity(query, keys)\n",
    "    weights = softmax(weights)\n",
    "    weighted_values = np.matmul(weights, values)\n",
    "    return weighted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_block_without_linear_layers(sent_embedding).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_block(sentence: np.ndarray):\n",
    "    #query\n",
    "    query = Tensor(sentence.copy())\n",
    "    query_layer = nn.Linear(1024, 1024)\n",
    "    query = query_layer(query)\n",
    "\n",
    "    #keys\n",
    "    keys = Tensor(sentence.copy())\n",
    "    keys_layer = nn.Linear(1024, 1024)\n",
    "    keys = keys_layer(keys)\n",
    "\n",
    "    #values\n",
    "    values = Tensor(sentence.copy())\n",
    "    values_layer = nn.Linear(1024, 1024)\n",
    "    values = values_layer(values)\n",
    "\n",
    "    weights = nn.CosineSimilarity()(query, keys)\n",
    "    weights = nn.Softmax(weights).dim\n",
    "    weighted_values = torch.matmul(weights, values)\n",
    "    return weighted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_block(sent_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, dim= 1024):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "\n",
    "        self.query_layer = nn.Linear(dim, dim)\n",
    "        self.keys_layer = nn.Linear(dim, dim)\n",
    "        self.values_layer = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "\n",
    "        if not isinstance(sentence, Tensor):\n",
    "            sentence= Tensor(sentence)\n",
    "\n",
    "        query= self.query_layer(sentence)\n",
    "        keys= self.keys_layer(sentence)\n",
    "        values= self.values_layer(sentence)\n",
    "\n",
    "        weights = F.cosine_similarity(query.unsqueeze(1), keys.unsqueeze(0), dim=-1)\n",
    "        weights = F.softmax(weights, dim= -1)\n",
    "\n",
    "        weighted_values = torch.matmul(weights, values)\n",
    "\n",
    "        return weighted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att= AttentionBlock()\n",
    "att.forward(sent_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi Head Attention Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self, dim= 1024, num_heads= 4):\n",
    "        super(MultiHeadAttentionBlock, self).__init__()\n",
    "\n",
    "        assert dim%num_heads==0, \"dim should be divisible by num_heads\"\n",
    "\n",
    "        self.dim= dim\n",
    "        self.heads = num_heads\n",
    "        self.per_head = dim // num_heads\n",
    "\n",
    "        self.query_layer = nn.Linear(dim, dim)\n",
    "        self.keys_layer = nn.Linear(dim, dim)\n",
    "        self.values_layer = nn.Linear(dim, dim)\n",
    "        self.linear_layer = nn.Linear(dim, dim)\n",
    "\n",
    "    def split_head(self, tensor: Tensor):\n",
    "        batch_size, num_tokens, dim = tensor.size()\n",
    "        return tensor.view(batch_size, num_tokens, self.heads, self.per_head).transpose(1, 2)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "\n",
    "        if not isinstance(sentence, Tensor):\n",
    "            sentence= Tensor(sentence)\n",
    "\n",
    "        query= self.split_head(self.query_layer(sentence))\n",
    "        keys= self.split_head(self.keys_layer(sentence))\n",
    "        values= self.split_head(self.values_layer(sentence))\n",
    "\n",
    "        weights = F.cosine_similarity(query.unsqueeze(3), keys.unsqueeze(2), dim=-1) / math.sqrt(self.per_head)\n",
    "        weights = F.softmax(weights, dim= -1)\n",
    "\n",
    "        weighted_values = torch.matmul(weights, values)\n",
    "        weighted_values = weighted_values.transpose(1,2).contiguous().reshape(sentence.shape[0], -1, self.dim)\n",
    "        attention = self.linear_layer(weighted_values)\n",
    "\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_embedding = np.random.rand(1, 10, 1024)\n",
    "mha = MultiHeadAttentionBlock()\n",
    "mha.forward(sent_embedding).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masked Multi-Head Attention Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWithMaskBlock(nn.Module):\n",
    "    def __init__(self, dim= 1024, num_heads= 4):\n",
    "        super(MultiHeadAttentionWithMaskBlock, self).__init__()\n",
    "\n",
    "        assert dim%num_heads==0, \"dim should be divisible by num_heads\"\n",
    "\n",
    "        self.dim= dim\n",
    "        self.heads = num_heads\n",
    "        self.per_head = dim // num_heads\n",
    "\n",
    "        self.query_layer = nn.Linear(dim, dim)\n",
    "        self.keys_layer = nn.Linear(dim, dim)\n",
    "        self.values_layer = nn.Linear(dim, dim)\n",
    "        self.linear_layer = nn.Linear(dim, dim)\n",
    "\n",
    "    def split_head(self, tensor: Tensor):\n",
    "        batch_size, num_tokens, dim = tensor.size()\n",
    "        return tensor.view(batch_size, num_tokens, self.heads, self.per_head).transpose(1, 2)\n",
    "\n",
    "    def forward(self, query, keys, values, mask= None):\n",
    "\n",
    "        query= self.split_head(self.query_layer(query))\n",
    "        keys= self.split_head(self.keys_layer(keys))\n",
    "        values= self.split_head(self.values_layer(values))\n",
    "\n",
    "        weights = F.cosine_similarity(query.unsqueeze(3), keys.unsqueeze(2), dim=-1) / math.sqrt(self.per_head)\n",
    "        if mask is not None:\n",
    "            weights = weights.masked_fill(mask == 0, -1e9)\n",
    "        weights = F.softmax(weights, dim= -2)\n",
    "\n",
    "        weighted_values = torch.matmul(weights, values)\n",
    "        weighted_values = weighted_values.transpose(1,2).contiguous().reshape(query.shape[0], -1, self.dim)\n",
    "        attention = self.linear_layer(weighted_values)\n",
    "\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_embedding = Tensor(np.random.rand(3, 10, 1024))\n",
    "sent= Tensor(np.random.rand(3, 10))\n",
    "sent[:, 8:] = 0\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(input, target):\n",
    "        input_mask = (input != 0).unsqueeze(1).unsqueeze(2)\n",
    "        target_mask = (target != 0).unsqueeze(1).unsqueeze(3)\n",
    "        sequence_length = input.size(1)\n",
    "\n",
    "        causal_mask = torch.tril(torch.ones(sequence_length, sequence_length), diagonal=0).bool()\n",
    "        causal_mask = causal_mask.unsqueeze(0).unsqueeze(1)\n",
    "\n",
    "        combined_mask = torch.logical_and(target_mask, ~causal_mask).to(input.device)\n",
    "        return input_mask, combined_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mask = (sent != 0).unsqueeze(1).unsqueeze(3)\n",
    "target_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = sent.size(1)\n",
    "causal_mask = torch.tril(torch.ones(sequence_length, sequence_length), diagonal=0).bool()\n",
    "causal_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (sent != 0).unsqueeze(1).unsqueeze(2)\n",
    "tri = torch.tril(torch.ones(3, 1, 10, 10))\n",
    "torch.logical_and(mask, tri).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mham = MultiHeadAttentionWithMaskBlock()\n",
    "res = mham.forward(sent_embedding, sent_embedding, sent_embedding, mask)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetworkBlock(nn.Module):\n",
    "    def __init__(self, dim= 1024, inter_dim= 512):\n",
    "        super(FeedForwardNetworkBlock, self).__init__()\n",
    "\n",
    "        self.ff1 = nn.Linear(dim, inter_dim)\n",
    "        self.ff2 = nn.Linear(inter_dim, dim)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, attention):\n",
    "        return self.ff2(self.relu(self.ff1(attention)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn = FeedForwardNetworkBlock()\n",
    "ffn.forward(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncodingBlock(nn.Module):\n",
    "    def __init__(self, max_token_length, dim):\n",
    "        super(PositionalEncodingBlock, self).__init__()\n",
    "\n",
    "        pe= torch.zeros(max_token_length, dim)\n",
    "        position = torch.arange(0, max_token_length, dtype=torch.float).unsqueeze(1)\n",
    "        # div_alt = torch.exp(torch.arange(0, dim, 2).float() * -(math.log(10000.0) / dim))\n",
    "        div = 1 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div)\n",
    "        pe[:, 1::2] = torch.cos(position * div)\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        return tokens + self.pe[:, :tokens.size(1), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe= PositionalEncodingBlock(100, 1024)\n",
    "pe.forward(sent_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, num_heads: int = 4, dim: int = 1024, inter_dim: int = 512):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttentionWithMaskBlock(dim, num_heads)\n",
    "        self.ff = FeedForwardNetworkBlock(dim, inter_dim)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, token_embeddings, input_mask):\n",
    "        mha_res = self.mha(token_embeddings, token_embeddings, token_embeddings, input_mask)\n",
    "        add_norm1= self.norm1(torch.add(token_embeddings, mha_res))\n",
    "        ff_res = self.ff(add_norm1)\n",
    "        add_norm2 = self.norm2(torch.add(add_norm1, ff_res))\n",
    "        return add_norm2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, num_heads: int = 4, dim: int = 1024, inter_dim: int = 512):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        self.self_attention = MultiHeadAttentionWithMaskBlock(dim, num_heads)\n",
    "        self.cross_attention = MultiHeadAttentionWithMaskBlock(dim, num_heads)\n",
    "        self.ff = FeedForwardNetworkBlock(dim, inter_dim)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.norm3 = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, output_embeddings, encoder_output, source_mask, target_mask):\n",
    "        mmha_res = self.self_attention(output_embeddings, output_embeddings, output_embeddings, target_mask)\n",
    "        add_norm1= self.norm1(torch.add(output_embeddings, mmha_res))\n",
    "        mha_res = self.cross_attention(add_norm1, encoder_output, encoder_output, source_mask)\n",
    "        add_norm2 = self.norm2(torch.add(mmha_res, mha_res))\n",
    "        ff_res = self.ff(add_norm2)\n",
    "        add_norm3 = self.norm2(torch.add(add_norm2, ff_res))\n",
    "        return add_norm3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_embedding = Tensor(np.random.randn(3, 10, 1024))\n",
    "sent= torch.rand(3, 10)\n",
    "enc_embedding = Tensor(np.random.randn(3, 12, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mask = (sent != 0).unsqueeze(1).unsqueeze(2)\n",
    "batch_size, sequence_length= sent.size()\n",
    "tri = torch.tril(torch.ones(batch_size, 1, sequence_length, sequence_length))\n",
    "torch.logical_and(target_mask, tri).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mask= torch.ones(1, 1, sent_embedding.size(1), 1)\n",
    "target_mask[:, :, out_embedding.size(1):, :] = 0\n",
    "source_mask= torch.zeros(1, 1, sent_embedding.size(1), 1)\n",
    "source_mask[:, :, out_embedding.size(1):, :] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec= DecoderBlock()\n",
    "dec(sent_embedding, sent_embedding, source_mask, target_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModule(nn.Module):\n",
    "    def __init__(self, dim, encoder_vocab_size, decoder_vocab_size, max_token_length, num_heads, num_layers):\n",
    "        super(TransformerModule, self).__init__()\n",
    "\n",
    "        self.encoder_embedding = nn.Embedding(encoder_vocab_size, dim)\n",
    "        self.decoder_embedding = nn.Embedding(decoder_vocab_size, dim)\n",
    "        self.positional_encoder = PositionalEncodingBlock(max_token_length, dim)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderBlock(num_heads, dim, 2* dim) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderBlock(num_heads, dim, 2* dim) for _ in range(num_layers)])\n",
    "        \n",
    "        self.linear = nn.Linear(dim, decoder_vocab_size)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def generate_mask(self, input, target):\n",
    "        input_mask = (input != 0).unsqueeze(1).unsqueeze(2)\n",
    "        target_mask = (target != 0).unsqueeze(1).unsqueeze(2)\n",
    "        batch_size, sequence_length= target.size()\n",
    "        tri = torch.tril(torch.ones(batch_size, 1, sequence_length, sequence_length))\n",
    "        target_mask = torch.logical_and(target_mask, tri).long()\n",
    "\n",
    "        return input_mask, target_mask\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        input_mask, target_mask = self.generate_mask(input, target)\n",
    "\n",
    "        input_embedding = self.encoder_embedding(input)\n",
    "        output_embedding = self.decoder_embedding(target)\n",
    "\n",
    "        encoder_output = input_embedding\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            encoder_output = encoder_layer(encoder_output, input_mask)\n",
    "\n",
    "        decoder_output = output_embedding\n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            decoder_output = decoder_layer(decoder_output, encoder_output, input_mask, target_mask)\n",
    "\n",
    "        linear_output = self.linear(decoder_output)\n",
    "        return self.softmax(linear_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TransformerModule(\n",
    "    dim= 64,\n",
    "    encoder_vocab_size=500,\n",
    "    decoder_vocab_size=500,\n",
    "    max_token_length=100,\n",
    "    num_heads=4,\n",
    "    num_layers=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentences = torch.randint(1, 500, (4, 100))\n",
    "target_sentences = torch.randint(1, 500, (4, 100))\n",
    "transformer(input_sentences, target_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
